{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# insert titel of code\n",
    "\n",
    "generic intro text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.args import *\n",
    "from scripts.base_model.preprocessing import *\n",
    "from scripts.base_model.train_base import *\n",
    "from scripts.base_model.models import *\n",
    "from scripts.evaluation.eval_model import *\n",
    "from scripts.evaluation.get_results import *\n",
    "from scripts.CEF_model.CEF_model import *\n",
    "from scripts.CEF_model.train_CEF import *\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### args preprocessing\n",
    "\n",
    "sentires_dir    =       location of the preprocessed data. \\\n",
    "review_dir      =       location of the json dataset \\\n",
    "user_thresh     =       how many reviews a user needs to have \\\n",
    "item_thresh     =       how many reviews an item has to have \\\n",
    "sample_ratio    =       \\\n",
    "test_length     =       how many items in the test set. \\\n",
    "neg_length      =       amount of negative items. \\\n",
    "save_path       =       where the dataset object will be saved. \\\n",
    "user_pre        =       wether or not to use a pre created dataset. If this value is true it will use                     the data stroted in save_path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parser_preprocessing():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--sentires_dir\", dest=\"sentires_dir\", type=str, default=\"data/input_data/reviews_with_features.txt\", \n",
    "                        help=\"path to sentires data\")\n",
    "    parser.add_argument(\"--review_dir\", dest=\"review_dir\", type=str, default=\"data/input_data/reviews_Electronics_5_filtered.json\", \n",
    "                        help=\"path to original review data\")\n",
    "    parser.add_argument(\"--user_thresh\", dest=\"user_thresh\", type=int, default=20, \n",
    "                        help=\"remove users with reviews less than this threshold\")\n",
    "    parser.add_argument(\"--item_thresh\", dest=\"item_thresh\", type=int, default=10, \n",
    "                        help=\"remove users with reviews less than this threshold\")\n",
    "    parser.add_argument(\"--sample_ratio\", dest=\"sample_ratio\", type=int, default=2, \n",
    "                        help=\"the (negative: positive sample) ratio for training BPR loss\")\n",
    "    parser.add_argument(\"--test_length\", dest=\"test_length\", type=int, default=5, \n",
    "                        help=\"the number of test items\")\n",
    "    parser.add_argument(\"--neg_length\", dest=\"neg_length\", type=int, default=100, help=\"# of negative samples in evaluation\")\n",
    "    parser.add_argument(\"--save_path\", dest=\"save_path\", type=str, default=\"data/preprocessed_data/dataset.pickle\", \n",
    "                        help=\"The path to save the preprocessed dataset object\")\n",
    "    parser.add_argument(\"--use_pre\", dest=\"use_pre\", type=str, default=False, \n",
    "            help=\"Wether or not to use a stored dataset object\")\n",
    "    parser.add_argument(\"--extra_filter\", dest=\"extra_filter\", type=bool, default=True)\n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### args Train base model\n",
    "\n",
    "device = either cpu or cuda depending on wether you use a gpu \\\n",
    "batch_size = the batch size \\\n",
    "lr = learning rate \\\n",
    "rec_k = length of the recommendation list \\\n",
    "weight_decay = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parser_training():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", dest = \"device\", type=str, default='cpu')\n",
    "    parser.add_argument(\"--gpu\", default=False)\n",
    "    parser.add_argument(\"--batch_size\", dest=\"batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--lr\", dest=\"lr\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--rec_k\", dest=\"rec_k\", type=int, default=5, help=\"length of rec list\")\n",
    "    parser.add_argument(\"--weight_decay\", default=0., type=float) # not sure whether to use\n",
    "    parser.add_argument(\"--model_path\", dest=\"model_path\", type=str, default=\"data/models/model.model\", \n",
    "                        help=\"The path to save the model\")\n",
    "    parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--use_pre\", dest=\"use_pre\", type=str, default=False, \n",
    "            help=\"Wether or not to use a stored model object\")\n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arg CEF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo\n",
    "def arg_parser_CEF():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", dest = \"device\", type=str, default='cpu')\n",
    "    parser.add_argument(\"--rec_k\", dest=\"rec_k\", type=int, default=5, help=\"length of rec list\")\n",
    "    parser.add_argument(\"--ld\", default=1, type=float) # not sure whether to use\n",
    "    parser.add_argument(\"--lr\", dest=\"lr\", type=float, default=0.01)\n",
    "    parser.add_argument(\"--model_path\", dest=\"model_path\", type=str, default=\"data/models/CEF_model.model\", \n",
    "                        help=\"The path to save the model\")\n",
    "    parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=100)\n",
    "    parser.add_argument(\"--use_pre\", dest=\"use_pre\", type=str, default=False, \n",
    "            help=\"Wether or not to use a stored model object\")\n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### args get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parser_results():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--device\", dest = \"device\", type=str, default='cpu')\n",
    "    parser.add_argument(\"--remove_size\", dest=\"remove_size\", type=int, default=50)\n",
    "    parser.add_argument(\"--rec_k\", dest=\"rec_k\", type=int, default=5, help=\"length of rec list\")\n",
    "    parser.add_argument(\"--output_path\", dest=\"output_path\", type=str, default=\"results/result dicts/\", \n",
    "                        help=\"The path to save the model\")\n",
    "    parser.add_argument(\"--epochs\", dest=\"epochs\", type=int, default=1000)\n",
    "    parser.add_argument(\"--beta\", dest=\"beta\", type=int, default=0.1 )\n",
    "    return parser.parse_known_args()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basemodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "np.random.seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the arguments for preprocessing\n",
    "preprocessing_args, unkown = arg_parser_preprocessing()\n",
    "# load dataset if the dataset exist\n",
    "dataset_path = preprocessing_args.save_path\n",
    "if preprocessing_args.use_pre:\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "else:\n",
    "    dataset = preprocessing(preprocessing_args)\n",
    "    with open(dataset_path, \"wb\") as f:\n",
    "            pickle.dump(dataset, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args, _ = arg_parser_training()\n",
    "model_path = train_args.model_path\n",
    "if train_args.use_pre:\n",
    "    base_model = BaseRecModel(dataset.feature_num, dataset).to(device)\n",
    "    base_model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    base_model = trainmodel(train_args, dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcg, f1, _ = eval_model(dataset, 5, base_model, device)\n",
    "print(f\"ndcg : {ndcg}\")\n",
    "print(f\"f1 : {f1}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CEF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stil to do.\n",
    "CEF_args, _ = arg_parser_CEF()\n",
    "model_path = CEF_args.model_path\n",
    "CEF_model = CEF(CEF_args, dataset, base_model).to(device)\n",
    "if CEF_args.use_pre:\n",
    "    CEF_model.load_state_dict(torch.load(model_path))\n",
    "else:\n",
    "    CEF_model = train_delta(CEF_args, CEF_model)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain a list of feature IDs, ranked by explainability score according to the trained CEF model (to clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usepre = False\n",
    "if usepre:\n",
    "    with open(\"results/ranked_ids.pickle\", \"rb\") as f:\n",
    "        ranked_ids = pickle.load(f)\n",
    "else:\n",
    "    with open(\"results/ranked_ids.pickle\", \"wb\") as f:\n",
    "        ranked_ids = CEF_model.top_k()\n",
    "        pickle.dump(ranked_ids, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_args, _ = arg_parser_results()\n",
    "with open(\"results/ranked_ids.pickle\", \"rb\") as f:\n",
    "    CEF_delete_list = pickle.load(f)\n",
    "CEF_delete_list.reverse()\n",
    "results = get_results(dataset, result_args, base_model,CEF_model, CEF_delete_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results):\n",
    "    for method in results:\n",
    "        result = results[method]\n",
    "        plt.plot(result[\"lt\"], result[\"ndcg\"], label = method)\n",
    "    \n",
    "    plt.xlabel(\"long tail rate\")\n",
    "    plt.ylabel(\"NDCG\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl2022",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ecadbf4e3aef346790049226dc91e9f34c4aab90566d2b23f967e0df41cfd1ee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
