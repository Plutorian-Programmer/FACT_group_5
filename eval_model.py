from evaluate_functions import *
from baselines import *
from utils import *
import torch
from models import BaseRecModel
from args import *
import pickle
from collections import defaultdict
import matplotlib.pyplot as plt
from CEF_model import *
from train_CEF import *
import copy
device = 'cpu'
dataset_path = "models/Dataset.pickle"
with open(dataset_path, "rb") as f:
    rec_dataset = pickle.load(f)

model_path = "models/model.model"
model = BaseRecModel(rec_dataset.feature_num, rec_dataset).to(device)
model.load_state_dict(torch.load(model_path))
k = 5

def run_tests(dataset, model):
    np.random.seed(42)
    e = 50
    k = 5
    device = "cpu"
    # CEF_model = CEF()
    # delta = train_delta(CEF_model)
    ids_to_delete = [1170,854,460,1478,405,750,155,52,456,1283,2086,477,2172,1811,1001,482,1368,578,2193,93,1375,1232,1891,1045,1019,1188,907,2092,1064,1544,1309,632,1122,1562,410,2006,520,647,171,621,90,968,418,1474,1336,2133,1159,1854,1817,1306,1981,33,1320,1210,1437,1272,1123,1667,443,868,412,276,391,1858,1192,548,1559,459,107,1206,625,1078,2079,99,1747,1757,123,952,903,1682,1255,505,1040,1871,1605,1302,2010,996,869,1977,1958,1804,1728,1093,1422,1360,158,15,1812,323,1153,960,1955,1114,664,1931,1128,1193,2021,1033,1866,1860,179,498,1108,1154,1573,1542,1433,1731,897,489,1942,295,771,221,522,479,18,1453,1184,329,708,1333,767,153,1825,746,290,692,2058,1683,1650,1139,538,2137,1612,226,525,1603,1578,1076,182,74,1392,877,1595,2084,237,231,1399,1439,1834,1187,1692,1967,1277,27,1645,1979,1273,526,586,612,1226,120,1161,135,2025,2159,690,1973,1755,1328,277,561,1990,1011,1517,696,392,1346,1428,1155,88,375,932,2140,1564,1322,139,1055,1924,70,1305,1588,1624,2170,1978,1799,255,2223,1355,763,600,1903,1928,2074,1832,939,2,244,1838,1282,2198,919,1625,917,2175,2034,681,1566,1631,1429,1331,1859,490,376,910,1137,1533,1824,567,1251,836,1780,494,254,1857,1264,1469,257,268,1325,1604,1049,1494,1808,667,995,518,1270,1602,1338,1851,350,938,1663,1005,1738,1502,193,790,1034,657,941,1672,1551,1704,1071,2169,334,65,2222,721,1031,857,1356,1662,1347,2120,1345,953,1940,1316,815,1500,835,1619,555,1694,1675,1279,146,2228,1423,1321,1475,2027,188,713,298,30,227,1409,666,1689,2195,13,1727,1057,198,1022,1661,67,1876,76,833,2087,1912,245,1993,416,920,485,1998,369,602,404,3,2205,1756,637,1007,347,1988,148,2007,983,1222,464,648,988,331,841,1622,1158,2050,445,1618,1935,165,2131,633,1898,800,1938,87,1608,151,2157,727,1644,1600,794,142,71,54,1039,1294,1933,1382,845,1795,660,1310,1038,1908,1497,2124,2199,53,264,1239,1334,1690,1558,453,1416,478,217,224,777,1765,41,96,814,1803,2174,1873,1537,791,511,628,1584,736,2181,285,894,930,942,1230,1739,964,1805,236,871,114,1238,493,427,737,1241,1495,1719,937,274,172,7,433,2171,1520,1706,1557,2016,29,1185,51,34,1491,832,901,1770,1513,528,1065,2068,2101,60,1269,303,745,420,1952,785,1579,811,1927,678,1281,475,1882,816,2208,569,1754,1180,1828,757,246,1771,1499,1101,337,999,340,108,594,649,926,1911,348,1779,377,640,259,599,1362,971,1736,1293,723,615,1845,695,1593,2035,1641,879,1591,1074,101,873,948,700,1581,2011,1783,251,977,267,143,253,136,1041,141,722,1577,684,1052,765,39,2149,1075,313,2165,2182,1468,796,552,1000,1798,413,1299,454,1477,173,2053,1081,202,2123,487,1151,1116,1350,1395,2151,634,305,507,1229,1702,1361,204,911,219,134,562,24,918,1659,1242,1145,1837,2173,1156,258,850,75,480,1652,1250,1199,1095,1405,1311,2135,754,956,133,966,718,974,1710,2072,1742,2142,1200,58,216,1300,2078,1032,451,878,1525,294,618,1637,1268,947,1920,2057,819,851,527,1458,1380,1587,786,394,2189,343,2162,514,1570,1945,1548,1265,1183,724,64,115,385,728,1802,201,592,2188,2145,1949,1878,1865,1420,1017,417,638,1569,1476,1698,1254,1377,1833,301,521,1890,1575,1190,89,1769,2082,180,1615,1292,1492,1385,2061,1349,852,206,1910,807,1290,1231,19,2067,1761,1353,230,2004,1256,9,731,944,688,1959,2110,1763,363,1221,373,256,297,1506,8,874,1220,880,2080,1810,2158,2047,1344,1561,1129,429,1681,1680,1974,402,1419,483,1351,1939,1088,169,190,1079,1699,2127,1791,81,818,630,646,1285,1259,1026,676,1162,749,2150,515,147,1053,187,1809,104,1280,381,218,2184,1213,1479,462,1786,2210,1317,899,1635,2226,2015,1567,1449,423,1693,2213,1717,1607,166,91,1194,949,2129,1646,366,1669,1815,1260,1195,775,421,675,1263,1104,961,1278,1656,891,1456,1142,924,614,1217,1174,336,967,149,125,248,424,469,156,315,407,1821,1842,225,1337,103,1853,575,778,14,733,1424,1543,1781,1922,2005,1359,435,2185,431,1536,1737,1540,327,950,702,608,1431,448,69,1209,972,47,1414,1100,1037,48,613,209,892,426,1287,1275,306,847,822,25,1215,2052,207,1008,1941,1141,1448,223,311,1173,1963,795,579,755,572,1,1696,1115,1068,934,1673,1181,296,506,2033,1485,610,2148,683,1848,1725,1823,1835,1885,1267,1386,2008,779,1010,923,626,639,982,2055,2109,2040,2215,669,11,697,471,1711,1135,1326,553,279,1196,1148,1839,1926,1237,401,411,978,1986,1528,645,1664,23,293,1144,1110,82,772,2168,2206,1648,164,1565,1530,2069,1590,916,1818,1167,1775,2073,240,1077,1004,508,1376,1447,20,357,959,239,1249,49,1841,2118,2136,152,627,1907,437,362,1298,234,1750,77,393,470,617,2063,1901,1862,2164,1636,2112,661,1893,1318,1512,1916,759,1035,1875,1043,1357,1994,1686,986,979,1103,1146,1384,1111,1203,570,2059,1168,1948,908,2049,1062,1563,2023,210,856,2209,390,325,1753,436,2098,1332,513,655,1985,2019,936,1975,1430,1383,770,1900,1904,481,1925,2166,1639,1473,1096,540,287,812,677,499,574,1685,358,829,2090,1113,1651,2152,921,2106,1671,43,1611,269,92,2197,261,1119,2220,2179,957,450,1133,884,386,1172,549,1895,1790,588,351,1549,1488,1462,1749,838,1972,1219,1745,1877,1951,1592,872,1571,591,1976,993,2012,1996,876,2085,422,282,1455,1806,1261,1946,349,1482,2065,929,50,1098,1816,1598,864,1487,658,644,1312,406,197,1840,1388,1820,365,2104,2099,1436,2054,1176,1552,250,1003,2180,1339,1697,516,2207,1768,1177,1902,1534,1295,706,1518,1274,1968,1984,384,2177,1296,1550,1647,278,159,447,1867,712,2108,2196,157,272,1709,2051,989,86,888,338,1870,1556,2201,875,2218,823,150,1289,289,1712,1751,1313,189,565,992,839,2096,1724,1850,2017,1404,1914,265,1813,1915,2146,5,858,1918,446,970,1140,1371,184,1729,1370,233,866,441,732,1441,398,940,1752,1394,154,782,2089,865,266,740,455,1524,2056,861,1258,1315,118,951,545,308,1880,1130,1047,1329,1223,21,1610,302,2095,62,1654,842,991,1228,1444,333,840,1778,45,1084,1117,1847,1628,1426,559,130,2212,817,719,10,717,1744,1149,205,2088,126,674,1766,509,1677,670,793,1759,716,140,524,1919,1582,2187,2211,1785,519,425,1391,110,1772,1746,758,828,252,66,1363,955,1969,1094,137,1284,1653,1792,1660,679,735,2160,307,503,589,1413,1314,280,1879,2183,788,699,304,654,1964,324,2190,1583,1703,370,781,1508,827,1147,1442,1120,1056,1467,663,332,2138,1308,585,495,1092,1109,768,789,1796,543,1205,1504,1197,741,476,1627,1830,1684,1881,1179,694,116,121,317,434,84,1486,138,1471,1992,2130,1807,1829,1378,656,318,962,309,512,1201,831,761,1658,1126,1446,1743,1323,1006,798,1970,560,607,145,1715,1013,214,912,1741,1526,922,260,1286,1892,558,531,1496,799,1980,2020,1457,808,1827,1401,927,2134,321,973,1800,2163,372,668,2024,2200,711,419,1016,199,2178,546,1348,1340,1212,1586,641,1632,792,1814,208,1118,72,1923,2214,389,1936,1691,281,764,709,94,356,1432,400,1965,1493,605,530,215,2048,844,650,59,1407,360,652,1519,144,1726,580,1535,834,439,1718,1670,1826,1913,1051,1002,2097,533,2071,1169,195,1152,1894,616,1665,954,631,804,1950,1396,319,1874,1072,780,830,859,1962,403,500,388,2081,1036,1944,715,2014,2046,2026,2100,1668,1445,914,701,665,774,597,2105,1687,2013,1454,163,1546,364,2191,2155,1246,883,653,1483,1434,2001,1576,1585,1029,2154,2161,1788,564,1341,1225,1124,1028,63,1461,249,1634,1061,461,22,220,1511,1734,769,2030,473,805,1023,965,1617,2091,491,2093,1909,355,1762,882,1171,1438,1410,1889,1621,2036,1849,1050,1509,686,898,1024,846,57,1427,2202,1247,1642,359,1545,1253,1107,1954,1789,1085,1248,1505,1720,2121,2204,1082,354,263,2066,958,1953,1083,452,1521,2064,1027,1989,1861,689,622,2192,378,2037,753,1218,813,1138,1777,160,1532,925,900,17,2122,1417,557,1614,1553,504,1868,2070,820,542,1327,1106,1393,1527,1623,1178,1708,35,1470,1574,1655,2216,181,1947,2042,2103,672,1995,1121,1245,2094,387,1412,1836,176,32,707,484,132,345,1235,1030,550,773,1059,643,1166,726,2029,1243,186,342,1897,596,1131,1202,2143,867,1666,1863,1330,2176,98,1886,399,1157,534,855,1080,1373,1257,1379,1906,1406,353,687,1440,620,748,270,1435,1090,1716,1291,1343,395,1018,1776,300,826,998,2116,783,109,1599,2221,554,367,744,529,725,1934,1288,766,371,1522,1408,37,161,734,609,1381,889,1831,346,1594,183,532,1252,1060,1740,2194,444,1905,2045,322,1073,438,1560,102,1271,1418,784,1303,893,714,862,1899,97,40,1369,1638,1896,382,863,1160,335,821,168,175,1465,465,2075,1887,1601,860,113,680,624,1484,2125,1982,275,604,312,990,501,79,83,606,1629,73,1971,825,584,119,1182,320,2139,286,1175,1501,1464,2227,38,449,472,1054,544,2126,1451,383,913,1784,581,738,760,203,1516,1733,541,2224,710,1937,593,16,623,466,1069,1421,162,177,409,2141,870,379,673,185,243,222,1555,262,1014,291,1797,1324,1186,1657,326,1335,551,810,1507,1367,1099,28,1207,651,1609,1127,747,1390,601,1855,729,316,1884,1262,341,809,330,976,1044,283,2062,1067,174,1991,2153,1046,1531,361,963,1966,2000,496,178,2115,1705,458,2039,566,1234,2156,408,1489,212,890,1782,587,2031,1015,1364,2038,1983,2022,242,61,975,2114,228,238,1773,1086,2203,194,2186,797,946,1012,235,1723,895,1760,1960,1529,12,705,299,1678,1572,1233,1930,611,933,247,985,1596,328,314,1730,2002,284,0,339,1398,510,1888,374,703,2003,2060,563,2144,1869,902,1105,196,310,292,440,1087,1620,1714,1568,1676,806,582,1943,191,881,502,776,1956,837,232,1514,1297,756,787,1143,590,583,1048,1389,442,1216,1515,352,849,396,1883,629,2147,1613,468,1767,1721,95,2076,1961,662,1764,1872,463,1674,698,1707,1240,1987,1460,1957,488,2119,170,415,1276,1793,1626,1354,6,853,1450,368,2043,1191,1112,1452,2225,2111,1089,1091,896,100,573,1643,1679,1224,523,539,1722,1020,2044,467,1597,4,127,2018,1097,124,428,1695,1358,1774,568,26,457,1748,801,1999,642,1538,1402,536,200,1342,1700,576,241,1864,492,1503,380,1063,2117,85,905,928,969,2032,192,943,68,704,1844,430,1649,44,1025,739,2132,848,122,1852,486,1132,720,1198,635,843,2077,2217,1204,603,36,2028,1163,1480,1580,595,167,1633,1374,935,111,1630,229,1481,1794,1319,2009,1411,46,213,981,1134,1058,55,273,1227,1236,1425,556,1400,1822,887,1997,636,1150,1917,685,1366,1523,1539,1758,909,2219,1125,931,414,762,915,1846,1466,2128,1164,571,2041,1136,1787,2107,1266,1365,1929,1732,1801,1498,2167,1403,802,288,1735,730,1856,112,131,1510,42,517,2102,1189,1819,1397,1372,1387,117,211,2083,1244,1490,1472,31,1070,1541,751,1208,344,78,129,693,1066,997,1554,671,1211,1463,2113,1640,271,691,535,1307,1165,1843,1932,1415,80,752,474,994,1921,987,1547,598,497,1042,1459,577,984,1606,1713,1021,980,1701,619,1352,397,128,1688,824,682,906,743,1214,945,659,1616,1102,1443,742,1301,105,1304,56,1589,547,106,886,537,885,904,803,432,1009]

    feature_count = min(dataset.feature_num//e, 1000)

    results = defaultdict(lambda : { "ndcg" : [], "lt" : [] } )
    
    random_dataset = baseline_random(dataset, 0)
    pop_item_dataset = baseline_pop(dataset, pop_method="item", e=0)
    pop_user_dataset = baseline_pop(dataset, pop_method="user", e=0)
    CEF_dataset = copy.deepcopy(dataset)
    visited_random = []

    ndcg, _ , lt = eval_model(dataset, k, model, device)
    results["random"]["ndcg"].append(ndcg)
    results["random"]["lt"].append(lt)
    results["pop_item"]["ndcg"].append(ndcg)
    results["pop_item"]["lt"].append(lt)
    results["pop_user"]["ndcg"].append(ndcg)
    results["pop_user"]["lt"].append(lt)
    results["CEF"]["ndcg"].append(ndcg)
    results["CEF"]["lt"].append(lt)
    
    progress = [i/10 for i in range(12)]
    for _ in range(feature_count):
        if progress != [] and _/feature_count >= progress[0]:
            print(progress.pop(0))
        # print(_)
        #random
        random_dataset = baseline_random(random_dataset, e=e, visited=visited_random)
        ndcg_random, _ , lt_random = eval_model(random_dataset, k, model, device)
        results["random"]["ndcg"].append(ndcg_random)
        results["random"]["lt"].append(lt_random)

        #pop item
        pop_item_dataset = baseline_pop(pop_item_dataset, pop_method="item", e=e)
        ndcg_item, _ , lt_item = eval_model(pop_item_dataset, k, model, device)
        results["pop_item"]["ndcg"].append(ndcg_item)
        results["pop_item"]["lt"].append(lt_item)

        #pop user
        pop_user_dataset = baseline_pop(pop_user_dataset, pop_method="user", e=e)
        ndcg_user, _ , lt_user = eval_model(pop_user_dataset, k, model, device)
        results["pop_user"]["ndcg"].append(ndcg_user)
        results["pop_user"]["lt"].append(lt_user)

        #CEF
        removal_list = ids_to_delete[:e]
        ids_to_delete = ids_to_delete[e:]
        CEF_dataset.remove_features(removal_list)
        ndcg_CEF, _ , lt_CEF = eval_model(CEF_dataset, k, model, device)
        results["CEF"]["ndcg"].append(ndcg_CEF)
        results["CEF"]["lt"].append(lt_CEF)

    results = dict(results)
    with open(f"results/ndcg_lt_{feature_count}_{e}.pickle", "wb") as f:
        pickle.dump(results, f)
    
    return results


def plot_results(results):
    for method in results:
        result = results[method]
        plt.scatter(result["lt"], result["ndcg"], label = method)
    
    plt.xlabel("long tail rate")
    plt.ylabel("NDCG")
    plt.legend()
    plt.show()

results = run_tests(rec_dataset, model)

# with open("results/ndcg_lt_20.pickle", "rb") as f:
#     results = pickle.load(f)
plot_results(results)
# print(rec_dataset.feature_num)
# feature_count_list = []
# feature_matrix = rec_dataset.user_feature_matrix
# feature_matrix = np.where(feature_matrix != 0, 1, 0)
# existence_array = np.sum(feature_matrix, axis=0)
# existence_array = np.sort(existence_array)

# plt.plot(existence_array)
# plt.show()
# print(len(ids_to_delete))